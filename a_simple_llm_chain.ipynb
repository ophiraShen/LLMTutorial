{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用 LCEL 构建一个简单的 LLM 应用程序\n",
    "\n",
    "在这个快速入门中，我们将用 LangChain 构建一个简单的 LLM 应用程序。\n",
    "\n",
    "这个应用程序将把文本从英语翻译成另一种语言。\n",
    "\n",
    "这是一个相对简单的 LLM 应用程序 - 它只是一次 LLM 调用加上一些提示。\n",
    "\n",
    "阅读完本教程后，您将对以下内容有一个高层次的概述：\n",
    "\n",
    "- 使用 **语言模型**\n",
    "- 使用 **PromptTemplates** 和 **OutputParsers**\n",
    "- 使用 **LangChain** 表达语言 **LCEL** 将组件链在一起\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装主要的 LangChain 库\n",
    "% pip install langchain\n",
    "\n",
    "# LangChain 核心库\n",
    "% pip install langchain-core\n",
    "\n",
    "# 语言模型\n",
    "% pip install langchain-openai\n",
    "\n",
    "# 尚未拆分为单独包的集成 langchain-community 包\n",
    "% pip install langchain-community\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用语言模型\n",
    "## 不用 **LangChain**, 直接调用模型接口"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI\n",
    "\n",
    "```shell\n",
    "pip install openai\n",
    "```\n",
    "\n",
    "配置 OpenAI API Key\n",
    "\n",
    "```shell\n",
    "export OPENAI_API_KEY=\"sk-proj-...\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\", refusal=None, role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Hello, how are you?\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepSeek\n",
    "\n",
    "```shell\n",
    "export DEEPSEEK_API_KEY=\"sk-...\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with whatever you need. How can I assist you today?\", refusal=None, role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "api_key = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "\n",
    "client = OpenAI(api_key=api_key, base_url=\"https://api.deepseek.com\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Hello, how are you?\"\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 LangChain 调用模型\n",
    "### OpenAI\n",
    "\n",
    "```shell\n",
    "pip install langchain-openai\n",
    "```\n",
    "\n",
    "```shell\n",
    "export OPENAI_API_KEY=\"sk-proj-...\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `ChatModel` 是 LangChain \"Runnables\" 的实例，这意味着它们暴露了一个标准接口供我们与之交互。\n",
    "- 要简单地调用模型，我们可以将消息列表传递给 `.invoke` 方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello! I'm just a program, but I'm here and ready to help you. How can I assist you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 13, 'total_tokens': 36, 'prompt_tokens_details': {'cached_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0ba0d124f1', 'finish_reason': 'stop', 'logprobs': None}, id='run-8a129c1d-5df0-4460-9a9d-fc1746b7f4a9-0', usage_metadata={'input_tokens': 13, 'output_tokens': 23, 'total_tokens': 36})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "openai_model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "response = openai_model.invoke([\"Hello, how are you?\"])\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Hello! I'm just a program, but I'm here and ready to help you. How can I assist you today?\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 13, 'total_tokens': 36, 'prompt_tokens_details': {'cached_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0ba0d124f1', 'finish_reason': 'stop', 'logprobs': None} id='run-8a129c1d-5df0-4460-9a9d-fc1746b7f4a9-0' usage_metadata={'input_tokens': 13, 'output_tokens': 23, 'total_tokens': 36}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepSeek\n",
    "\n",
    "```shell\n",
    "export DEEPSEEK_API_KEY=\"sk-...\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 36, 'prompt_tokens': 9, 'total_tokens': 45, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 9}, 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_1c141eb703', 'finish_reason': 'stop', 'logprobs': None}, id='run-577ce806-9d1b-4028-bb00-5bdf7216abea-0', usage_metadata={'input_tokens': 9, 'output_tokens': 36, 'total_tokens': 45})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "api_key = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "\n",
    "deepseek_model = ChatOpenAI(\n",
    "    model=\"deepseek-chat\",\n",
    "    openai_api_key=api_key,\n",
    "    openai_api_base='https://api.deepseek.com'\n",
    ")\n",
    "\n",
    "response = deepseek_model.invoke([\"Hello, how are you?\"])\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 36, 'prompt_tokens': 9, 'total_tokens': 45, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 9}, 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_1c141eb703', 'finish_reason': 'stop', 'logprobs': None} id='run-577ce806-9d1b-4028-bb00-5bdf7216abea-0' usage_metadata={'input_tokens': 9, 'output_tokens': 36, 'total_tokens': 45}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Messages\n",
    "\n",
    "- 消息是 LangChain 中用于与 LLM 交互的主要数据结构。\n",
    "- 它们是包含角色（如user、assistant或system）和内容（如文本）的字典列表。\n",
    "\n",
    "    - `HumanMessage` 用户发送的消息\n",
    "    - `AIMessage` LLM 发送的消息\n",
    "    - `SystemMessage` 系统消息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='你好，你好吗？', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 24, 'total_tokens': 28, 'prompt_tokens_details': {'cached_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0ba0d124f1', 'finish_reason': 'stop', 'logprobs': None}, id='run-ec08491c-eba7-4048-8889-08a84faac466-0', usage_metadata={'input_tokens': 24, 'output_tokens': 4, 'total_tokens': 28})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"Translate the following from English into Chinese\"),\n",
    "    HumanMessage(content=\"Hello, how are you?\"),\n",
    "]\n",
    "\n",
    "response = openai_model.invoke(messages)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='你好，你好吗？', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 16, 'total_tokens': 21, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 16}, 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_1c141eb703', 'finish_reason': 'stop', 'logprobs': None}, id='run-681d9bfd-2f08-4d61-8ffc-cd372651a58f-0', usage_metadata={'input_tokens': 16, 'output_tokens': 5, 'total_tokens': 21})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = deepseek_model.invoke(messages)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OutputParser 输出解析器\n",
    "\n",
    "模型的响应是一个 `AIMessage`。这包含一个字符串响应以及有关响应的其他元数据。通常我们可能只想处理字符串响应。我们可以通过使用简单的输出解析器来解析出这个响应。\n",
    "- 输出解析器是 LangChain 中用于解析 LLM 输出的组件。\n",
    "- 它们将 LLM 的输出转换为更有用的格式，例如字符串、JSON、Python 对象等。\n",
    "- 输出解析器在 LangChain 中广泛使用，用于处理和操作 LLM 的输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你好，你好吗？'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "parser.invoke(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你好，你好吗？'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PromptTemplate 提示模板\n",
    "\n",
    "现在我们直接将消息列表传递给语言模型。这些消息列表来自哪里？通常，它是由用户输入和应用程序逻辑的组合构建的。此应用程序逻辑通常将原始用户输入转换为准备传递给语言模型的消息列表。常见的转换包括添加系统消息或使用用户输入格式化模板。\n",
    "\n",
    "PromptTemplates 是 LangChain 中的一个概念，旨在帮助进行这种转换。它们接受原始用户输入并返回数据（提示），这些数据准备好传递给语言模型。\n",
    "\n",
    "让我们在这里创建一个 PromptTemplate。它将接受两个用户变量：\n",
    "\n",
    "- `language`: 要翻译文本的语言\n",
    "- `text`: 要翻译的文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='Translate the following into Chinese:', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello, how are you?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "prompt_template = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\", \"Translate the following into {language}:\"),\n",
    "        (\"user\", \"{text}\")\n",
    "    ]\n",
    ")\n",
    "result = prompt_template.invoke(\n",
    "    {\n",
    "        \"language\": \"Chinese\",\n",
    "        \"text\": \"Hello, how are you?\"\n",
    "    }\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='Translate the following into Chinese:', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello, how are you?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以看到它返回一个 `ChatPromptValue`，由两条消息组成。\n",
    "如果我们想直接访问消息，我们可以这样做："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='Translate the following into Chinese:', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello, how are you?', additional_kwargs={}, response_metadata={})]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Translate the following into Chinese:', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Hello, how are you?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(result.messages)\n",
    "result.to_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用 LCEL 将组件链在一起\n",
    "\n",
    "我们现在可以使用管道（`|`）运算符将其与上面的提示模板、模型和输出解析器结合起来：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你好，你好吗？'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt_template | openai_model | parser\n",
    "\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"language\": \"Chinese\",\n",
    "        \"text\": \"Hello, how are you?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你好，你好吗？'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt_template | deepseek_model | parser\n",
    "\n",
    "chain.invoke({\"language\": \"Chinese\", \"text\": \"Hello, how are you?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openaiquickstart",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
